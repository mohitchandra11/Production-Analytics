<p align="center">
  <img src="assets/project_banner.png" width="100%" alt="Arcelor Mittal Steel Production Data Engineering Pipeline and Operations Analytics Banner">
</p>

<p align="center">

# ğŸ­ Arcelor Mittal Hot Rolling Plant Production  Analytics: End-to-End Data Engineering & BI Solution

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Azure](https://img.shields.io/badge/Azure-Data_Factory-0078D4.svg)](https://azure.microsoft.com/)
[![SQL](https://img.shields.io/badge/SQL-Azure_SQL-CC2927.svg)](https://azure.microsoft.com/en-us/products/azure-sql/database/)
[![Power BI](https://img.shields.io/badge/Power_BI-Dashboards-F2C811.svg)](https://powerbi.microsoft.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Real-world industrial data engineering project that delivered a 10% tempo improvement and 5% reduction in maintenance downtime, enabling consistent achievement of monthly production targets at ArcelorMittal Vanderbijlpark Works.**

---

## ğŸ“Š **Business Impact at a Glance**

| Metric | Result | Business Value |
|--------|--------|---------------|
| **Tempo Improvement** | ~10% increase | Identified and resolved bottlenecks through data-driven insights |
| **Maintenance Downtime** | ~5% reduction | Targeted problem equipment with predictive maintenance triggers |
| **Shift Optimization** | 4-shift â†’ 3-shift recommendation | Reduced shift handover losses by 15+ minutes |
| **Production Targets** | Consistently met/exceeded | First time achieving monthly targets post-Saldanha closure |
| **Manual Reporting** | 40% reduction | Automated weekly performance reporting for management |
| **ROI** | Measurable gains | Enabled 30% production increase without capital investment |

---

## ğŸ¯ **Problem Statement**

### **The Business Challenge**

After the closure of ArcelorMittal's Saldanha Works, **all thin flat products were redirected to Vanderbijlpark**, increasing monthly production targets by **30%**. The temper lineâ€”the final step before dispatchâ€”was:

- âŒ Not originally designed for thin flat products
- âŒ Operating without historical data for the new product mix
- âŒ Experiencing falling tempo (throughput)
- âŒ Suffering from frequent breakdowns and inconsistent performance
- âŒ Missing monthly production targets
- âŒ Unable to explain delays through operator feedback alone

**Management needed answers:** What's slowing down the line? Which equipment is the bottleneck? Why do shifts vary in performance? How can we hit our targets?

### **The Data Engineering Solution**

I built a complete **end-to-end data engineering and analytics pipeline** that:
1. Extracted production and maintenance data from multiple sources
2. Engineered features to model equipment-level coil processing cycles
3. Deployed scalable pipelines on **Azure Data Factory**
4. Stored transformed data in **Azure SQL Database** for analytics
5. Built **Power BI dashboards** that became the primary weekly performance tool

---

## ğŸ—ï¸ **Architecture Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES (AMSA)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Production Data (MES System - 13,575 records)                        â”‚
â”‚  â€¢ Maintenance Downtime Logs (1,450 events, 113 equipment types)        â”‚
â”‚  â€¢ Level-1 Encoder Signals (Equipment activity - synthetic)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PYTHON DATA ENGINEERING PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Data Extraction & Validation                                        â”‚
â”‚     â””â”€ Load CSV, handle encoding (UTF-8-sig), parse timestamps          â”‚
â”‚                                                                          â”‚
â”‚  2. Data Cleaning & Transformation                                      â”‚
â”‚     â”œâ”€ Filter date ranges (April-August 2024)                           â”‚
â”‚     â”œâ”€ Clean equipment names (remove trailing patterns)                 â”‚
â”‚     â”œâ”€ Parse maintenance durations (hours/minutes)                      â”‚
â”‚     â””â”€ Handle missing values and outliers                               â”‚
â”‚                                                                          â”‚
â”‚  3. Feature Engineering                                                 â”‚
â”‚     â”œâ”€ Prime vs Scrap classification (HL/HM/98 vs HX/HY/HZ)            â”‚
â”‚     â”œâ”€ Parent-child coil relationships (CID â†’ UID mapping)              â”‚
â”‚     â”œâ”€ Gap analysis (tempo measurement between coils)                   â”‚
â”‚     â”œâ”€ Product mix complexity scoring                                   â”‚
â”‚     â””â”€ Bottleneck identification (6 critical equipment pieces)          â”‚
â”‚                                                                          â”‚
â”‚  4. Synthetic Cycle Time Modeling                                       â”‚
â”‚     â”œâ”€ Anchor operations to REAL MES completion timestamps              â”‚
â”‚     â”œâ”€ Work backwards to build equipment operation timeline             â”‚
â”‚     â”œâ”€ Apply product-specific multipliers (thin vs thick)               â”‚
â”‚     â”œâ”€ Factor in shift performance (A/B/C/D crews)                      â”‚
â”‚     â””â”€ Generate RUN/IDLE/FAULT event sequences                          â”‚
â”‚                                                                          â”‚
â”‚  5. Dimensional Modeling (Star Schema)                                  â”‚
â”‚     â”œâ”€ dim_equipment (17 production pieces, process order)              â”‚
â”‚     â”œâ”€ dim_date_crew_schedule (4-crew rotation, day/night)              â”‚
â”‚     â”œâ”€ fact_production_coil (real completion times + gaps)              â”‚
â”‚     â”œâ”€ fact_maintenance_event (parsed downtime events)                  â”‚
â”‚     â”œâ”€ fact_coil_operation_cycle (equipment-level operations)           â”‚
â”‚     â””â”€ fact_equipment_event_log (RUN/IDLE/FAULT timeline)               â”‚
â”‚                                                                          â”‚
â”‚  OUTPUT: 8 CSV files (6 engineered + 2 raw reference)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AZURE DATA FACTORY (ADF)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Copy Data Activities (CSV â†’ Azure SQL)                               â”‚
â”‚  â€¢ Data Flow Transformations (schema mapping)                           â”‚
â”‚  â€¢ Pipeline Orchestration (scheduled runs)                              â”‚
â”‚  â€¢ Incremental Load Strategy (append new data)                          â”‚
â”‚  â€¢ Error Handling & Logging                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AZURE SQL DATABASE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Star Schema Implementation                                            â”‚
â”‚  â€¢ Indexed for Query Performance                                         â”‚
â”‚  â€¢ Views for Complex Aggregations                                        â”‚
â”‚  â€¢ Stored Procedures for KPI Calculations                                â”‚
â”‚  â€¢ Row-Level Security (RLS) for Shift Supervisors                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         POWER BI DASHBOARDS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Executive Summary     â†’ High-level KPIs for management              â”‚
â”‚  2. Bottleneck Analysis   â†’ Equipment constraints & time contributions   â”‚
â”‚  3. Maintenance & Downtime â†’ MTBF, MTTR, reliability tracking            â”‚
â”‚  4. Shift Performance     â†’ Crew comparison & handover analysis          â”‚
â”‚  5. Product Mix & Tempo   â†’ How product characteristics affect flow      â”‚
â”‚                                                                          â”‚
â”‚  â€¢ 150+ DAX Measures                                                     â”‚
â”‚  â€¢ Real-time refresh (hourly during production)                          â”‚
â”‚  â€¢ Mobile-optimized layouts                                              â”‚
â”‚  â€¢ Drill-through pages for deep-dive analysis                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ **Technical Implementation**

### **Phase 1: Python Data Engineering Pipeline**

**Technologies:** Python 3.9, Pandas, NumPy

#### **Key Engineering Achievements:**

1. **Data Integration from Multiple Sources**
   - Production data: 13,575 coil records from MES system
   - Maintenance data: 1,450 downtime events across 113 equipment types
   - Equipment metadata: 17 production equipment pieces with process sequencing

2. **Advanced Feature Engineering**
   - **Prime/Scrap Classification:** Automated categorization (HL/HM/98 vs HX/HY/HZ)
   - **Parent-Child Coil Tracking:** Mapped CID (parent) â†’ UID (output pieces) relationships
   - **Tempo Analysis:** Calculated gaps between completions to measure line throughput
   - **Bottleneck Identification:** Flagged 6 critical equipment pieces consuming 42% of line time

3. **Synthetic Cycle Time Modeling** â­ **(Key Innovation)**
   - **Challenge:** No equipment-level encoder data available (NDA restrictions)
   - **Solution:** Built synthetic operation timeline anchored to REAL MES completion timestamps
   - **Methodology:**
     - Work backwards from actual completion time
     - Apply product-specific duration multipliers (thin: 0.5-0.7Ã—, thick: 1.1-1.3Ã—)
     - Factor in shift performance variations (Shift A: 1.05Ã—, C: 0.95Ã—)
     - Generate equipment operation start/end times per coil
     - Validate: Synthetic end_datetime matches real completion_ts (error: <1 second)

4. **Dimensional Data Modeling**
   - Star schema design with 2 dimension tables + 4 fact tables
   - Optimized for analytical queries (OLAP)
   - Enables slice-and-dice analysis by date, shift, equipment, product type

#### **Pipeline Scripts:**

```python
# Example: Core pipeline structure
1. load_data_updated.py              # Data extraction & validation
2. filter_april_august.py            # Date range filtering
3. clean_subarea.py                  # Equipment name standardization
4. build_dim_equipment.py            # Equipment dimension with process order
5. build_fact_production_coil.py     # Production facts with real timestamps
6. build_fact_maintenance_event.py   # Maintenance event parsing
7. build_crew_rotation.py            # 4-crew shift schedule (A/B/C/D)
8. duration_queue_logic.py           # Product-specific cycle time rules
9. generate_operations_anchored.py   # Synthetic operations (KEY INNOVATION)
10. build_equipment_event_log.py     # RUN/IDLE/FAULT timeline generation
11. clean_gaps.py                    # Outlier removal & gap analysis
12. validation_analysis.py           # Data quality checks
13. export_tables.py                 # Export to 8 CSV files
```

**Output:** 8 production-ready CSV files (6 engineered + 2 raw reference)

---
<p align="center">
  <img src="assets/Master_Load_Pipeline.png" width="100%" alt="Arcelor Mittal Steel Production Data Engineering Pipeline Azure Data Factory">
</p>

<p align="center">

### **Phase 2: Azure Data Factory Deployment**

**Technologies:** Azure Data Factory, Azure SQL Database, T-SQL

#### **Data Pipeline Architecture:**

1. **Source Configuration**
   - Blob Storage: Staged CSV files from Python pipeline
   - Linked Services: Secure connection to Azure SQL Database
   - Datasets: Schema definitions for all 8 tables

2. **Copy Data Activities**
   - **Production Data Pipeline:** `fact_production_coil` (13,575 rows)
   - **Maintenance Pipeline:** `fact_maintenance_event` (1,450 rows)
   - **Equipment Cycles Pipeline:** `fact_coil_operation_cycle` (230,775 operations)
   - **Equipment Events Pipeline:** `fact_equipment_event_log` (RUN/IDLE/FAULT)
   - **Dimension Tables:** `dim_equipment`, `dim_date_crew_schedule`

3. **Data Transformation in ADF**
   - Schema mapping (CSV â†’ SQL)
   - Data type conversions (datetime, decimal, boolean)
   - Null handling strategies
   - Incremental load logic (append-only for fact tables)

4. **Orchestration & Scheduling**
   - Trigger: Time-based (hourly during production hours: 06:00-18:00)
   - Dependencies: Sequential execution (dimensions â†’ facts)
   - Error handling: Retry logic, failure notifications
   - Logging: Activity monitoring, performance metrics

5. **SQL Database Design**
   ```sql
   -- Star schema in Azure SQL Database
   
   -- Dimension Tables
   CREATE TABLE dim_equipment (
       equipment_id INT PRIMARY KEY,
       equipment_name NVARCHAR(100),
       section NVARCHAR(20),
       process_order INT,
       is_bottleneck_candidate BIT,
       is_active BIT
   );
   
   CREATE TABLE dim_date_crew_schedule (
       production_date DATE PRIMARY KEY,
       day_crew CHAR(1),
       night_crew CHAR(1),
       week_number INT,
       month_name NVARCHAR(20)
   );
   
   -- Fact Tables
   CREATE TABLE fact_production_coil (
       coil_id NVARCHAR(50) PRIMARY KEY,
       parent_coil_id NVARCHAR(50),
       completion_ts DATETIME2,
       production_date DATE,
       shift_code CHAR(1),
       type_code NVARCHAR(10),
       is_prime BIT,
       is_scrap BIT,
       thickness_mm DECIMAL(5,2),
       width_mm DECIMAL(6,2),
       mass_out_tons DECIMAL(8,3),
       total_cycle_time_min DECIMAL(8,2),
       gap_from_prev_completion_min DECIMAL(8,2),
       gap_from_prev_parent_min DECIMAL(8,2),
       FOREIGN KEY (production_date) REFERENCES dim_date_crew_schedule(production_date)
   );
   
   -- Additional fact tables: maintenance, cycles, events
   -- Indexed on: equipment_id, production_date, shift_code, type_code
   ```

6. **Performance Optimization**
   - Clustered indexes on primary keys
   - Non-clustered indexes on foreign keys and filter columns
   - Columnstore indexes for analytical queries
   - Query performance monitoring

---

### **Phase 3: SQL Analytics Layer**

**Technologies:** T-SQL, Azure SQL Database, Stored Procedures

#### **Analytical Views & Queries:**

```sql
-- Example: Bottleneck Analysis View
CREATE VIEW vw_equipment_bottleneck_analysis AS
SELECT 
    e.equipment_name,
    e.section,
    e.is_bottleneck_candidate,
    COUNT(DISTINCT c.coil_id) AS total_coils_processed,
    SUM(c.operation_duration_sec) / 3600.0 AS total_operation_hours,
    AVG(c.operation_duration_sec) / 60.0 AS avg_operation_time_min,
    SUM(c.operation_duration_sec) / SUM(SUM(c.operation_duration_sec)) 
        OVER () * 100 AS time_share_pct
FROM dim_equipment e
JOIN fact_coil_operation_cycle c ON e.equipment_id = c.equipment_id
WHERE e.is_active = 1
GROUP BY e.equipment_name, e.section, e.is_bottleneck_candidate
ORDER BY time_share_pct DESC;

-- Example: Shift Performance KPIs
CREATE PROCEDURE sp_calculate_shift_kpis
    @start_date DATE,
    @end_date DATE
AS
BEGIN
    SELECT 
        p.shift_code,
        COUNT(p.coil_id) AS total_pieces,
        COUNT(CASE WHEN p.is_prime = 1 THEN 1 END) AS prime_pieces,
        CAST(COUNT(CASE WHEN p.is_prime = 1 THEN 1 END) AS FLOAT) 
            / COUNT(p.coil_id) * 100 AS prime_rate_pct,
        AVG(p.total_cycle_time_min) AS avg_cycle_time_min,
        60.0 / AVG(p.total_cycle_time_min) AS pieces_per_hour,
        AVG(p.gap_from_prev_completion_min) AS avg_completion_gap_min
    FROM fact_production_coil p
    WHERE p.production_date BETWEEN @start_date AND @end_date
    GROUP BY p.shift_code
    ORDER BY pieces_per_hour DESC;
END;
```

---

### **Phase 4: Power BI Dashboards**

**Technologies:** Power BI Desktop, DAX, Power Query

#### **5 Production Dashboards:**

1. **Executive Summary**
   - KPIs: Total Pieces, Prime Rate %, Tempo, Equipment Utilization
   - Charts: Daily tempo trend, product mix, shift comparison
   - Alerts: Top 5 equipment issues

2. **Bottleneck Analysis**
   - Equipment time contribution waterfall
   - Bottleneck severity matrix
   - Operation vs idle time scatter plot

3. **Maintenance & Downtime**
   - MTBF, MTTR reliability metrics
   - Downtime by equipment (top 10)
   - MTBF vs MTTR quadrant analysis

4. **Shift Performance**
   - 4 shift comparison cards (A, B, C, D)
   - Shift efficiency trends over time
   - Handover loss analysis (15+ min identified)

5. **Product Mix & Tempo**
   - Width vs thickness scatter (cycle time bubbles)
   - Cycle time distribution by product type
   - Parent coil yield analysis

#### **DAX Measures (150+ Total):**

```dax
// Example: Tempo Target Achievement
Tempo Achievement % = 
VAR CurrentTempo = [Pieces per Hour]
VAR Target = [Tempo Target]
RETURN
    DIVIDE(CurrentTempo, Target, 0) * 100

// Example: Equipment Bottleneck Score
Equipment Bottleneck Score = 
VAR EquipmentOpsTime = SUM(fact_coil_operation_cycle[operation_duration_sec])
VAR TotalOpsTime = 
    CALCULATE(
        SUM(fact_coil_operation_cycle[operation_duration_sec]),
        ALL(dim_equipment)
    )
VAR TimeShare = DIVIDE(EquipmentOpsTime, TotalOpsTime, 0)
VAR IsBottleneck = MAX(dim_equipment[is_bottleneck_candidate])
RETURN
    IF(IsBottleneck, TimeShare * 100, TimeShare * 50)

// Example: MTBF (Mean Time Between Failures)
MTBF = 
VAR TotalProductionHours = [Total RUN Hours] + [Total IDLE Hours]
VAR FailureCount = [Unplanned Events]
RETURN
    DIVIDE(TotalProductionHours, FailureCount, 0)
```

---

## ğŸ“ˆ **Business Impact & Results**

### **Quantified Improvements**

| Area | Metric | Improvement | How Achieved |
|------|--------|-------------|--------------|
| **Production Throughput** | Tempo (pcs/hr) | **~10% increase** | Identified and resolved 3 critical bottlenecks (Temper Mill, Exit Coil Car, Decoiler) |
| **Equipment Reliability** | Maintenance Downtime | **~5% reduction** | Targeted maintenance on problem equipment, MTBF/MTTR tracking |
| **Operational Efficiency** | Shift Handover Loss | **Reduced by 15+ min** | Data-driven recommendation: 4-shift â†’ 3-shift pattern |
| **Production Targets** | Monthly Target Achievement | **Consistently met/exceeded** | First time achieving targets post-Saldanha closure (30% increase) |
| **Reporting Efficiency** | Manual Reporting Time | **40% reduction** | Automated weekly performance dashboards replaced manual Excel reports |
| **Decision Making** | Time to Identify Issues | **Real-time vs weekly** | Live dashboards enable immediate action on equipment failures |

### **Strategic Recommendations Implemented**

Based on data insights, I provided the following recommendations that were acted upon:

1. âœ… **Move to 3-shift pattern** â†’ Reduced shift handover losses
2. âœ… **Bi-weekly planned maintenance shutdowns** â†’ Proactive vs reactive maintenance
3. âœ… **Automate exit-zone handling** â†’ Reduced manual coil handling delays
4. âœ… **Add temporary offloading personnel** â†’ Addressed exit bottleneck
5. ğŸ“‹ **Build predictive maintenance models** â†’ Roadmap for next phase
6. ğŸ“‹ **Integrate live PLC/SCADA signals** â†’ Real-time cycle tracking (future enhancement)

### **User Adoption & Impact**

- **Daily Active Users:** Plant Manager, Production Manager, 4 Shift Supervisors, Maintenance Manager, Process Engineers
- **Usage Pattern:** Dashboard became the **primary weekly performance review tool**
- **Feedback:** "Finally we can see what's really happening on the line" - Production Manager
- **Business Outcome:** Performance-based incentives introduced due to improved operational consistency

---

## ğŸ—‚ï¸ **Project Structure**

```
hot-rolling-plant-analytics/
â”‚
â”œâ”€â”€ data/                                    # Data files (sanitized for portfolio)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ coil_production_april_august_2024.csv
â”‚   â”‚   â””â”€â”€ maintenance_downtime_jna_Oct_2024.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ dim_equipment.csv
â”‚       â”œâ”€â”€ dim_date_crew_schedule.csv
â”‚       â”œâ”€â”€ fact_production_coil.csv
â”‚       â”œâ”€â”€ fact_maintenance_event.csv
â”‚       â”œâ”€â”€ fact_coil_operation_cycle.csv
â”‚       â””â”€â”€ fact_equipment_event_log.csv
â”‚
â”œâ”€â”€ python_pipeline/                         # Python ETL scripts
â”‚   â”œâ”€â”€ 01_load_data_updated.py
â”‚   â”œâ”€â”€ 02_filter_april_august.py
â”‚   â”œâ”€â”€ 03_clean_subarea.py
â”‚   â”œâ”€â”€ 04_build_dim_equipment.py
â”‚   â”œâ”€â”€ 05_build_fact_production_coil.py
â”‚   â”œâ”€â”€ 06_build_fact_maintenance_event.py
â”‚   â”œâ”€â”€ 07_build_crew_rotation.py
â”‚   â”œâ”€â”€ 08_duration_queue_logic.py
â”‚   â”œâ”€â”€ 09_generate_operations_anchored.py
â”‚   â”œâ”€â”€ 10_build_equipment_event_log.py
â”‚   â”œâ”€â”€ 11_clean_gaps.py
â”‚   â”œâ”€â”€ 12_validation_analysis.py
â”‚   â””â”€â”€ 13_export_tables.py
â”‚
â”œâ”€â”€ azure_data_factory/                      # ADF pipeline definitions
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ pipeline_production_data.json
â”‚   â”‚   â”œâ”€â”€ pipeline_maintenance_data.json
â”‚   â”‚   â””â”€â”€ pipeline_equipment_cycles.json
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â”œâ”€â”€ dataset_csv_source.json
â”‚   â”‚   â””â”€â”€ dataset_sql_sink.json
â”‚   â””â”€â”€ linked_services/
â”‚       â”œâ”€â”€ ls_azure_blob_storage.json
â”‚       â””â”€â”€ ls_azure_sql_database.json
â”‚
â”œâ”€â”€ sql/                                     # SQL scripts
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â”œâ”€â”€ create_dimensions.sql
â”‚   â”‚   â”œâ”€â”€ create_facts.sql
â”‚   â”‚   â””â”€â”€ create_indexes.sql
â”‚   â”œâ”€â”€ views/
â”‚   â”‚   â”œâ”€â”€ vw_equipment_bottleneck_analysis.sql
â”‚   â”‚   â”œâ”€â”€ vw_shift_performance_summary.sql
â”‚   â”‚   â””â”€â”€ vw_maintenance_reliability.sql
â”‚   â””â”€â”€ stored_procedures/
â”‚       â”œâ”€â”€ sp_calculate_shift_kpis.sql
â”‚       â”œâ”€â”€ sp_calculate_mtbf_mttr.sql
â”‚       â””â”€â”€ sp_identify_bottlenecks.sql
â”‚
â”œâ”€â”€ powerbi/                                 # Power BI files
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ dashboard_1_executive_summary.pbix
â”‚   â”‚   â”œâ”€â”€ dashboard_2_bottleneck_analysis.pbix
â”‚   â”‚   â”œâ”€â”€ dashboard_3_maintenance_downtime.pbix
â”‚   â”‚   â”œâ”€â”€ dashboard_4_shift_performance.pbix
â”‚   â”‚   â””â”€â”€ dashboard_5_product_mix_tempo.pbix
â”‚   â”œâ”€â”€ theme/
â”‚   â”‚   â”œâ”€â”€ ArcelorMittal_PowerBI_Theme.json
â”‚   â”‚   â””â”€â”€ ArcelorMittal_PowerBI_Theme_Documentation.txt
â”‚   â”œâ”€â”€ dax_formulas/
â”‚   â”‚   â””â”€â”€ DAX_Formulas_Reference.txt
â”‚   â””â”€â”€ prototypes/
â”‚       â”œâ”€â”€ dashboard_1_executive_summary_with_charts.html
â”‚       â”œâ”€â”€ dashboard_2_bottleneck_analysis.html
â”‚       â”œâ”€â”€ dashboard_3_maintenance_downtime.html
â”‚       â”œâ”€â”€ dashboard_4_shift_performance.html
â”‚       â””â”€â”€ dashboard_5_product_mix_tempo.html
â”‚
â”œâ”€â”€ docs/                                    # Documentation
â”‚   â”œâ”€â”€ PowerBI_Implementation_Guide.txt
â”‚   â”œâ”€â”€ DAX_Formulas_Reference.txt
â”‚   â”œâ”€â”€ Azure_Data_Factory_Setup_Guide.md
â”‚   â””â”€â”€ SQL_Database_Schema_Documentation.md
â”‚
â”œâ”€â”€ notebooks/                               # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 02_bottleneck_identification.ipynb
â”‚   â””â”€â”€ 03_model_validation.ipynb
â”‚
â”œâ”€â”€ requirements.txt                         # Python dependencies
â”œâ”€â”€ .gitignore                              
â”œâ”€â”€ LICENSE                                 
â””â”€â”€ README.md                               # This file
```

---

## ğŸš€ **Getting Started**

### **Prerequisites**

- Python 3.9+
- Azure Account (for ADF and SQL Database)
- Power BI Desktop
- Git

### **Installation**

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/hot-rolling-plant-analytics.git
   cd hot-rolling-plant-analytics
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Python ETL pipeline**
   ```bash
   cd python_pipeline
   python 01_load_data_updated.py
   python 02_filter_april_august.py
   # ... continue with all scripts in order
   python 13_export_tables.py
   ```

4. **Deploy to Azure Data Factory**
   - Import pipeline JSON files from `azure_data_factory/pipelines/`
   - Configure linked services (Blob Storage, SQL Database)
   - Set up triggers for scheduled runs

5. **Set up Azure SQL Database**
   ```bash
   cd sql/schema
   # Run scripts in order:
   sqlcmd -S yourserver.database.windows.net -U username -P password -i create_dimensions.sql
   sqlcmd -S yourserver.database.windows.net -U username -P password -i create_facts.sql
   sqlcmd -S yourserver.database.windows.net -U username -P password -i create_indexes.sql
   ```

6. **Open Power BI dashboards**
   ```bash
   cd powerbi/dashboards
   # Open .pbix files in Power BI Desktop
   # Update data source connections to your Azure SQL Database
   # Apply ArcelorMittal theme from powerbi/theme/
   ```

---

## ğŸ› ï¸ **Technologies Used**

### **Data Engineering**
- **Python 3.9:** Core ETL pipeline development
- **Pandas:** Data manipulation and transformation
- **NumPy:** Numerical computations
- **Azure Data Factory:** Cloud-based data orchestration
- **Azure Blob Storage:** Staging area for CSV files

### **Data Storage & Analytics**
- **Azure SQL Database:** Relational data warehouse (star schema)
- **T-SQL:** Stored procedures, views, and analytical queries

### **Business Intelligence**
- **Power BI Desktop:** Interactive dashboards
- **DAX:** Advanced calculations and KPIs
- **Power Query:** Data transformation layer

### **Development Tools**
- **Git/GitHub:** Version control
- **Jupyter Notebooks:** Exploratory data analysis
- **VS Code:** IDE for Python development

---

## ğŸ“Š **Key Technical Highlights**

### **1. Synthetic Cycle Time Modeling (Novel Approach)**

**Challenge:** Equipment encoder data unavailable due to NDA restrictions.

**Solution:** Engineered a synthetic operation timeline that:
- âœ… Anchors to **REAL MES completion timestamps** (100% accurate)
- âœ… Works backwards to estimate equipment-level durations
- âœ… Applies product-specific multipliers (thin: 0.5-0.7Ã—, thick: 1.1-1.3Ã—)
- âœ… Factors in shift performance variations
- âœ… Validates with <1 second error margin

**Why This Matters:** Enables equipment-level bottleneck analysis without proprietary sensor data.

### **2. Parent-Child Coil Tracking**

- Mapped 8,008 parent coils (CID) â†’ 13,575 output pieces (UID)
- Average yield: 1.7 pieces per parent coil
- Prime rate: 87.2% (target: 85%)
- Enabled yield optimization analysis

### **3. Tempo Analysis Through Gap Measurement**

- **Completion Gap:** Time between any two consecutive pieces (line throughput)
- **Parent Gap:** Time between parent coils (tempo measurement)
- Identified 15+ minute shift handover losses
- Enabled 4-shift â†’ 3-shift recommendation

### **4. Scalable Azure Architecture**

- **Automated:** Hourly data refresh during production hours
- **Scalable:** Can handle 10Ã— data volume without redesign
- **Secure:** Row-level security for shift-specific data access
- **Cost-effective:** Serverless SQL, pay-per-use ADF

---

## ğŸ“– **Documentation**

Comprehensive documentation available in the `docs/` folder:

- **PowerBI_Implementation_Guide.txt:** Step-by-step dashboard setup (50+ pages)
- **DAX_Formulas_Reference.txt:** Complete DAX measure library (150+ measures)
- **Azure_Data_Factory_Setup_Guide.md:** ADF pipeline deployment instructions
- **SQL_Database_Schema_Documentation.md:** Database design and query examples

---

## ğŸ“ **Skills Demonstrated**

### **Data Engineering**
âœ… ETL pipeline development (Python)  
âœ… Cloud data orchestration (Azure Data Factory)  
âœ… Dimensional modeling (star schema)  
âœ… Data quality validation  
âœ… Feature engineering  
âœ… Synthetic data generation  

### **Data Analytics**
âœ… SQL analytics (T-SQL, stored procedures, views)  
âœ… Statistical analysis (bottleneck identification)  
âœ… Time-series analysis (trend detection)  
âœ… KPI definition and tracking  

### **Business Intelligence**
âœ… Dashboard design (5 production dashboards)  
âœ… DAX advanced calculations (150+ measures)  
âœ… Data visualization best practices  
âœ… User experience design  

### **Cloud Technologies**
âœ… Azure Data Factory (pipelines, triggers, monitoring)  
âœ… Azure SQL Database (indexing, performance tuning)  
âœ… Azure Blob Storage (data staging)  

### **Business Acumen**
âœ… Stakeholder communication (management, engineers, operators)  
âœ… ROI analysis and quantification  
âœ… Strategic recommendations  
âœ… Change management (4-shift â†’ 3-shift transition)  

---

## ğŸ¤ **Contributing**

This is a portfolio project showcasing real-world data engineering work. However, if you'd like to suggest improvements or report issues:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit your changes (`git commit -m 'Add improvement'`)
4. Push to the branch (`git push origin feature/improvement`)
5. Open a Pull Request

---

## ğŸ“§ **Contact**

**[Timothy Tshimauswu]**  
Data  Scientist | Business Intelligence Analyst  

- ğŸ“§ Email: timothytshimauswu@gmail.com
- ğŸ’¼ LinkedIn: [linkedin.com/in/yourprofile](https://linkedin.com/in/utshimauswu/)
- ğŸ™ GitHub: [github.com/yourusername](https://github.com/TimothyTshimauswu)
- ğŸ“Š Portfolio: [[yourportfolio.com](https://cloud-data-ai-portfolio-landing.vercel.app/)

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ **Acknowledgments**

- **ArcelorMittal Vanderbijlpark Works** for providing the business context and real production data
- **Process Engineering Team** for domain expertise and validation
- **Production & Maintenance Teams** for adopting the solution and providing feedback

---

## ğŸ“š **Additional Resources**

- [Power BI Theme Documentation](docs/ArcelorMittal_PowerBI_Theme_Documentation.txt)
- [Interactive Dashboard Prototypes](powerbi/prototypes/)
- [SQL Query Examples](sql/views/)
- [Python Pipeline Scripts](python_pipeline/)

---

<div align="center">

**â­ If this project helped you, please consider giving it a star! â­**

</div>
